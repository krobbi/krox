from lox_error_reporter import ErrorReporter
from lox_token import Token
from lox_token_type import TokenType
from typing import Any, Self

class Scanner:
    """ Scans a list of tokens from source code. """
    
    KEYWORDS: dict[str, TokenType] = {
        "and": TokenType.AND,
        "class": TokenType.CLASS,
        "else": TokenType.ELSE,
        "false": TokenType.FALSE,
        "for": TokenType.FOR,
        "fun": TokenType.FUN,
        "if": TokenType.IF,
        "nil": TokenType.NIL,
        "or": TokenType.OR,
        "print": TokenType.PRINT,
        "return": TokenType.RETURN,
        "super": TokenType.SUPER,
        "this": TokenType.THIS,
        "true": TokenType.TRUE,
        "var": TokenType.VAR,
        "while": TokenType.WHILE,
    }
    """ A map of lexemes to keyword token types. """
    
    error_reporter: ErrorReporter
    """ The scanner's error reporter. """
    
    source: str
    """ The source code to scan. """
    
    tokens: list[Token]
    """ The tokens generated by the scanner. """
    
    start: int = 0
    """ The index of the current lexeme's first character. """
    
    current: int = 0
    """ The index of the current lexeme's current character. """
    
    line: int = 1
    """ The current line number. """
    
    def __init__(
            self: Self, error_reporter: ErrorReporter, source: str) -> None:
        """ Initialize the scanner. """
        
        self.error_reporter = error_reporter
        self.source = source
        self.tokens = []
    
    
    def scan_tokens(self: Self) -> list[Token]:
        """ Scan a list of tokens from the source code. """
        
        while not self.is_at_end():
            self.start = self.current
            self.scan_token()
        
        self.tokens.append(Token(TokenType.EOF, "", None, self.line))
        return self.tokens
    
    
    def scan_token(self: Self) -> None:
        """ Scan a token from the source code. """
        
        c: str = self.advance()
        
        match c:
            case "(":
                self.add_token(TokenType.LEFT_PAREN)
            case ")":
                self.add_token(TokenType.RIGHT_PAREN)
            case "{":
                self.add_token(TokenType.LEFT_BRACE)
            case "}":
                self.add_token(TokenType.RIGHT_BRACE)
            case ",":
                self.add_token(TokenType.COMMA)
            case ".":
                self.add_token(TokenType.DOT)
            case "-":
                self.add_token(TokenType.MINUS)
            case "+":
                self.add_token(TokenType.PLUS)
            case ";":
                self.add_token(TokenType.SEMICOLON)
            case "*":
                self.add_token(TokenType.STAR)
            case "!":
                if self.match("="):
                    self.add_token(TokenType.BANG_EQUAL)
                else:
                    self.add_token(TokenType.BANG)
            case "=":
                if self.match("="):
                    self.add_token(TokenType.EQUAL_EQUAL)
                else:
                    self.add_token(TokenType.EQUAL)
            case "<":
                if self.match("="):
                    self.add_token(TokenType.LESS_EQUAL)
                else:
                    self.add_token(TokenType.LESS)
            case ">":
                if self.match("="):
                    self.add_token(TokenType.GREATER_EQUAL)
                else:
                    self.add_token(TokenType.GREATER)
            case "/":
                if self.match("/"):
                    while self.peek() != "\n" and not self.is_at_end():
                        self.advance()
                else:
                    self.add_token(TokenType.SLASH)
            case " " | "\r" | "\t":
                pass # Ignore whitespace.
            case "\n":
                self.line += 1
            case '"':
                self.string()
            case _:
                if self.is_digit(c):
                    self.number()
                elif self.is_alpha(c):
                    self.identifier()
                else:
                    self.error_reporter.report(
                            self.line, "Unexpected character.")
    
    
    def identifier(self: Self) -> None:
        """ Generate an identifier literal or keyword token. """
        
        while self.is_alpha_numeric(self.peek()):
            self.advance()
        
        text: str = self.source[self.start:self.current]
        type: TokenType = self.KEYWORDS.get(text, TokenType.IDENTIFIER)
        self.add_token(type)
    
    
    def number(self: Self) -> None:
        """ Generate a number literal token. """
        
        while self.is_digit(self.peek()):
            self.advance()
        
        if self.peek() == "." and self.is_digit(self.peek_next()):
            self.advance()
            
            while self.is_digit(self.peek()):
                self.advance()
        
        self.add_token(
                TokenType.NUMBER, float(self.source[self.start:self.current]))
    
    
    def string(self: Self) -> None:
        """ Generate a string literal token. """
        
        while self.peek() != '"' and not self.is_at_end():
            if self.peek() == "\n":
                self.line += 1
            
            self.advance()
        
        if self.is_at_end():
            self.error_reporter.report(self.line, "Unterminated string.")
            return
        
        self.advance() # Consume the closing `"`.
        
        value: str = self.source[self.start + 1:self.current - 1]
        self.add_token(TokenType.STRING, value)
    
    
    def match(self: Self, expected: str) -> bool:
        """
        Consume the current character if it matches an expected
        character and return whether the current character was consumed.
        """
        
        if self.is_at_end():
            return False
        
        if self.source[self.current] == expected:
            self.current += 1
            return True
        else:
            return False
    
    
    def peek(self: Self) -> str:
        """ Return the current character without consuming it. """
        
        if self.is_at_end():
            return "\0"
        else:
            return self.source[self.current]
    
    
    def peek_next(self: Self) -> str:
        """ Return the next character without consuming it. """
        
        if self.current + 1 >= len(self.source):
            return "\0"
        else:
            return self.source[self.current + 1]
    
    
    def is_alpha(self: Self, c: str) -> bool:
        """
        Return whether a chracter is an alphabetical character or an
        underscore.
        """
        
        code: int = ord(c)
        
        return (
            code >= ord("a") and code <= ord("z")
            or code >= ord("A") and code <= ord("Z")
            or code == ord("_"))
    
    
    def is_alpha_numeric(self: Self, c: str) -> bool:
        """
        Return whether a character is an alphabetical character,
        underscore, or digit.
        """
        
        return self.is_alpha(c) or self.is_digit(c)
    
    
    def is_digit(self: Self, c: str) -> bool:
        """ Return whether a character is a digit. """
        
        code: int = ord(c)
        return code >= ord("0") and code <= ord("9")
    
    
    def is_at_end(self: Self) -> bool:
        """ Return whether all of the characters have been consumed. """
        
        return self.current >= len(self.source)
    
    
    def advance(self: Self) -> str:
        """ Consume and return the current character. """
        
        character: str = self.source[self.current]
        self.current += 1
        return character
    
    
    def add_token(self: Self, type: TokenType, literal: Any = None) -> None:
        """ Generate a new token from its type and literal. """
        
        text: str = self.source[self.start:self.current]
        self.tokens.append(Token(type, text, literal, self.line))
